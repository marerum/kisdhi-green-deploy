/**
 * HearingInput Component
 * Provides a text area interface for inputting hearing content with manual save functionality
 * Supports both creating new logs and editing existing ones
 * Includes voice input functionality using Azure Speech Service (primary) and Web Speech API (fallback)
 */

'use client';

import { useState, useEffect, useRef } from 'react';
import { hearingApi } from '@/lib/api';
import { HearingLogResponse } from '@/types/api';
import { useAzureSpeech } from '@/hooks/useAzureSpeech';

// Web Speech API type declarations
declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
  }
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start(): void;
  stop(): void;
  onresult: (event: SpeechRecognitionEvent) => void;
  onerror: (event: SpeechRecognitionErrorEvent) => void;
  onend: () => void;
  onstart: () => void;
}

interface SpeechRecognitionEvent {
  resultIndex: number;
  results: SpeechRecognitionResultList;
}

interface SpeechRecognitionErrorEvent {
  error: string;
}

interface SpeechRecognitionResultList {
  length: number;
  [index: number]: SpeechRecognitionResult;
}

interface SpeechRecognitionResult {
  isFinal: boolean;
  [index: number]: SpeechRecognitionAlternative;
}

interface SpeechRecognitionAlternative {
  transcript: string;
  confidence: number;
}

declare var SpeechRecognition: {
  prototype: SpeechRecognition;
  new(): SpeechRecognition;
};

// ============================================
// ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: å‹å®šç¾©
// ============================================
type TriggerMode = 'auto' | 'manual';
type AutoTriggerTimeout = 2000 | 3000 | 4000 | 5000 | 6000 | 7000 | 8000 | 9000 | 10000;
type AutoStopTimeout = null | 60000; // null = åœæ­¢ç„¡ã—, 60000 = 1åˆ†ã§åœæ­¢

interface TriggerSettings {
  mode: TriggerMode;
  timeout: AutoTriggerTimeout | null;
}

const DEFAULT_TRIGGER_SETTINGS: TriggerSettings = {
  mode: 'auto',
  timeout: 3000
};

const MIN_TEXT_LENGTH = 20;
// ============================================
  
interface HearingInputProps {
  projectId: number;
  onHearingLogAdded?: (hearingLog: HearingLogResponse) => void;
  onHearingLogUpdated?: (hearingLog: HearingLogResponse) => void;
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ: è¦ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ãƒ•ãƒ­ãƒ¼æ›´æ–°ã‚’é€šçŸ¥ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  onFlowUpdated?: (flow: any) => void;
  editingLog?: HearingLogResponse | null;
  onCancelEdit?: () => void;
}

export function HearingInput({ 
  projectId, 
  onHearingLogAdded, 
  onHearingLogUpdated,
  onFlowUpdated,  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ: ãƒ•ãƒ­ãƒ¼æ›´æ–°ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  editingLog,
  onCancelEdit
}: HearingInputProps) {
  const [content, setContent] = useState('');
  const [isSaving, setIsSaving] = useState(false);
  const [error, setError] = useState<string | null>(null);
  
  // Voice input states (only real-time recognition)
  const [isTranscribing, setIsTranscribing] = useState(false);

  // Speech recognition provider state ('azure' or 'web')
  const [speechProvider, setSpeechProvider] = useState<'azure' | 'web'>('azure');

  // Azure Speech Service integration
  const azureSpeech = useAzureSpeech({
    language: 'ja-JP',
    onTranscript: (text, isFinal) => {
      if (isFinal) {
        // ç¢ºå®šçµæœ: baseContentã«è¿½åŠ 
        baseContentRef.current = baseContentRef.current + text;
        setContent(baseContentRef.current);
        lastSpeechTimeRef.current = Date.now();
      } else {
        // ä¸­é–“çµæœ: baseContent + ä¸­é–“çµæœã‚’è¡¨ç¤º
        setContent(baseContentRef.current + text);
      }
    },
    onError: (error) => {
      console.error('Azure Speech error:', error);
      setError(`éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${error}`);
    },
  });

  // Real-time speech recognition states (Web Speech API fallback)
  const [isRealtimeListening, setIsRealtimeListening] = useState(false);
  const isRealtimeListeningRef = useRef<boolean>(false); // refã§ã‚‚ç®¡ç†ï¼ˆã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£å•é¡Œå¯¾ç­–ï¼‰
  const [recognition, setRecognition] = useState<SpeechRecognition | null>(null);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const baseContentRef = useRef<string>(''); // Content before starting real-time recognition

  // ============================================
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: çŠ¶æ…‹ç®¡ç†
  // ============================================
  const [triggerSettings, setTriggerSettings] = useState<TriggerSettings>(DEFAULT_TRIGGER_SETTINGS);
  const [pendingText, setPendingText] = useState('');
  const [currentFlow, setCurrentFlow] = useState<any>(null);
  const [isGeneratingFlow, setIsGeneratingFlow] = useState(false);
  const lastUpdateTimeRef = useRef<number>(Date.now());
  const triggerTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  // æœ€å¾Œã«ãƒ•ãƒ­ãƒ¼ç”Ÿæˆã—ãŸæ™‚ç‚¹ã®contentã®é•·ã•ï¼ˆã“ã‚Œä»¥é™ãŒpendingTextï¼‰
  const lastFlowGenerationPositionRef = useRef<number>(0);
  // éŸ³å£°å…¥åŠ›è‡ªå‹•åœæ­¢è¨­å®š
  const [autoStopTimeout, setAutoStopTimeout] = useState<AutoStopTimeout>(null);
  const lastSpeechTimeRef = useRef<number>(Date.now());
  // éŸ³å£°å…¥åŠ›ä¸­ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°ãƒ­ã‚°è‡ªå‹•ä¿å­˜ç”¨
  const currentHearingLogIdRef = useRef<number | null>(null);
  // ============================================
  
  useEffect(() => {
    if (editingLog) setContent(editingLog.content);
    else setContent('');
  }, [editingLog]);

  // Update contentRef when content changes
  useEffect(() => {
    // No longer needed since we removed realtime functionality
  }, [content]);

    // Cleanup on unmount only (no dependencies to prevent premature cleanup)
  useEffect(() => {
    return () => {
      // Stop real-time recognition
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: ã‚¿ã‚¤ãƒãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
      if (triggerTimeoutRef.current) {
        clearTimeout(triggerTimeoutRef.current);
      }
    };
  }, []); // Empty dependency array - cleanup only on unmount

  // ============================================
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: éŸ³å£°èªè­˜çµæœã®ç›£è¦–
  // ============================================
  useEffect(() => {
    if (!isRealtimeListening) return;

    // æœ€å¾Œã®ãƒ•ãƒ­ãƒ¼ç”Ÿæˆä½ç½®ä»¥é™ãŒpendingText
    const newContent = content.substring(lastFlowGenerationPositionRef.current);
    if (newContent) {
      setPendingText(newContent);
      lastUpdateTimeRef.current = Date.now();

      console.log('=== Flow Generation Trigger Check ===');
      console.log('Trigger mode:', triggerSettings.mode);
      console.log('Trigger timeout:', triggerSettings.timeout);
      console.log('New content:', newContent);
      console.log('Pending text length:', newContent.length);
      console.log('Min required length:', MIN_TEXT_LENGTH);

      // ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
      if (triggerTimeoutRef.current) {
        clearTimeout(triggerTimeoutRef.current);
      }

      // è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾Œã«ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ
      if (triggerSettings.mode === 'auto' && triggerSettings.timeout) {
        const timeoutMs = triggerSettings.timeout;
        
        console.log(`Setting flow generation timer for ${timeoutMs}ms`);
        
        triggerTimeoutRef.current = setTimeout(() => {
          const timeSinceUpdate = Date.now() - lastUpdateTimeRef.current;
          const currentPendingText = content.substring(lastFlowGenerationPositionRef.current);
          
          console.log('=== Flow Generation Timer Fired ===');
          console.log('Time since update:', timeSinceUpdate);
          console.log('Current pending text:', currentPendingText);
          console.log('Current pending text length:', currentPendingText.length);
          console.log('Condition met:', timeSinceUpdate >= timeoutMs && currentPendingText.length >= MIN_TEXT_LENGTH);
          
          if (timeSinceUpdate >= timeoutMs && currentPendingText.length >= MIN_TEXT_LENGTH) {
            console.log('Calling handleIncrementalFlowGeneration');
            handleIncrementalFlowGeneration();
          } else {
            console.log('Flow generation conditions not met');
          }
        }, timeoutMs);
      }
    }
  }, [content, isRealtimeListening, triggerSettings.mode, triggerSettings.timeout]); // pendingTextã‚’å‰Šé™¤
  // ============================================

  const isEditing = !!editingLog;

  // ============================================
  // ãƒ’ã‚¢ãƒªãƒ³ã‚°ãƒ­ã‚°è‡ªå‹•ä¿å­˜å‡¦ç†
  // ============================================
  const autoSaveHearingLog = async (textToSave: string, finalize: boolean = false) => {
    const contentToSave = textToSave.trim();
    
    if (!contentToSave) return;
    
    try {
      const logId = currentHearingLogIdRef.current;
      if (logId) {
        // æ—¢å­˜ã®ãƒ­ã‚°ã‚’æ›´æ–°
        const updated = await hearingApi.updateHearingLog(logId, { content: contentToSave });
        
        if (finalize) {
          // ãƒ•ãƒ­ãƒ¼ç”Ÿæˆå¾Œã®ç¢ºå®šä¿å­˜ã®å ´åˆã€è¦ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«é€šçŸ¥
          onHearingLogUpdated?.(updated);
          // æ¬¡ã®ç™ºè¨€ç”¨ã«ãƒªã‚»ãƒƒãƒˆ
          currentHearingLogIdRef.current = null;
        }
      } else {
        // æ–°è¦ä½œæˆï¼ˆåˆã‚ã¦ã®éŸ³å£°èªè­˜ã¾ãŸã¯ãƒ•ãƒ­ãƒ¼ç”Ÿæˆå¾Œã®æ¬¡ã®ç™ºè¨€æ™‚ï¼‰
        const created = await hearingApi.addHearingLog(projectId, { content: contentToSave });
        currentHearingLogIdRef.current = created.id;
        
        if (finalize) {
          // ç¢ºå®šä¿å­˜ã®å ´åˆã®ã¿è¦ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«é€šçŸ¥
          onHearingLogAdded?.(created);
          // æ¬¡ã®ç™ºè¨€ç”¨ã«ãƒªã‚»ãƒƒãƒˆ
          currentHearingLogIdRef.current = null;
        }
      }
    } catch (error) {
      console.error('Auto-save failed:', error);
      // ã‚¨ãƒ©ãƒ¼ã¯æ§ãˆã‚ã«ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯è¦‹ã›ãªã„ï¼‰
    }
  };
  // ============================================

    // ============================================
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: å¢—åˆ†ãƒ•ãƒ­ãƒ¼ç”Ÿæˆå‡¦ç†
  // ============================================
  const handleIncrementalFlowGeneration = async () => {
    if (!pendingText.trim() || isGeneratingFlow) return;

    setIsGeneratingFlow(true);
    try {
      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: APIå‘¼ã³å‡ºã—ã‚’å®Ÿè£…
      const response = await hearingApi.generateIncrementalFlow(
        projectId,
        currentFlow,
        pendingText,
        content
      );
      
      // ãƒ•ãƒ­ãƒ¼ã‚’æ›´æ–°
      setCurrentFlow(response.flow);
      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ: è¦ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆpage.tsxï¼‰ã«ãƒ•ãƒ­ãƒ¼æ›´æ–°ã‚’é€šçŸ¥
      // ã“ã‚Œã«ã‚ˆã‚ŠFlowPreviewã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›´æ–°ã•ã‚Œã‚‹
      onFlowUpdated?.(response.flow);
      
      // ãƒ•ãƒ­ãƒ¼ç”ŸæˆæˆåŠŸå¾Œã€ç¾åœ¨ã®pendingTextã‚’ãƒ’ã‚¢ãƒªãƒ³ã‚°ãƒ­ã‚°ã¨ã—ã¦ç¢ºå®šä¿å­˜
      // finalize=trueã§å‘¼ã³å‡ºã—ã€pendingTextã®ã¿ã‚’ä¿å­˜ã—ã¦ãƒ­ã‚°ã‚’ç¢ºå®š
      await autoSaveHearingLog(pendingText, true);
      
      // æˆåŠŸã—ãŸã‚‰ pendingText ã‚’ã‚¯ãƒªã‚¢ã—ã€æ¬¡ã®ç™ºè¨€ã‚’æ–°è¦ãƒ­ã‚°ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ãŸã‚ã®æº–å‚™
      setPendingText('');
      setContent('');
      baseContentRef.current = '';
      lastFlowGenerationPositionRef.current = 0;
      
      // éŸ³å£°èªè­˜ãŒåœæ­¢ã—ã¦ã„ã‚‹å ´åˆã¯å†èµ·å‹•
      if (isRealtimeListeningRef.current && !recognitionRef.current) {
        // å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†èµ·å‹•
        setTimeout(() => {
          if (isRealtimeListeningRef.current) {
            startRealtimeListening();
          }
        }, 500);
      }
      
    } catch (error) {
      console.error('Flow generation failed:', error);
      setError('ãƒ•ãƒ­ãƒ¼ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ');
    } finally {
      setIsGeneratingFlow(false);
    }
  };

  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: æ‰‹å‹•ãƒ•ãƒ­ãƒ¼ç”Ÿæˆãƒœã‚¿ãƒ³ã®ãƒãƒ³ãƒ‰ãƒ©
  const handleManualFlowGeneration = () => {
    if (triggerSettings.mode === 'manual' && pendingText.trim()) {
      handleIncrementalFlowGeneration();
    }
  };
  // ============================================

  const startRealtimeListening = async () => {
    try {
      setError(null);
      
      // æ—¢ã«éŸ³å£°èªè­˜ãŒå®Ÿè¡Œä¸­ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
      if ((speechProvider === 'azure' && azureSpeech.isListening) || 
          (speechProvider === 'web' && (recognitionRef.current || isRealtimeListeningRef.current))) {
        console.log('Speech recognition already running, skipping start');
        return;
      }
      
      // ç·¨é›†ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯æ—¢å­˜ã®IDã‚’ä½¿ç”¨
      if (editingLog) {
        currentHearingLogIdRef.current = editingLog.id;
      }
      
      // Store the current content as base content
      baseContentRef.current = content;
      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ: é–‹å§‹æ™‚ã®ãƒ•ãƒ­ãƒ¼ç”Ÿæˆä½ç½®ã‚’è¨­å®š
      lastFlowGenerationPositionRef.current = content.length;
      // éŸ³å£°å…¥åŠ›è‡ªå‹•åœæ­¢: æœ€å¾Œã®éŸ³å£°æ™‚é–“ã‚’åˆæœŸåŒ–
      lastSpeechTimeRef.current = Date.now();
      
      // Azure Speech Service ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆ
      if (azureSpeech.isAvailable && speechProvider === 'azure') {
        console.log('Starting Azure Speech Service recognition...');
        azureSpeech.startListening();
        setIsRealtimeListening(true);
        isRealtimeListeningRef.current = true;
        return;
      }
      
      // Fallback to Web Speech API
      console.log('Falling back to Web Speech API...');
      setSpeechProvider('web');
      
      // Check if Web Speech API is supported
      const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition;
      if (!SpeechRecognition) {
        setError('ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚Chromeã€Edgeã€Safariç­‰ã®æœ€æ–°ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚');
        return;
      }
      
      const recognitionInstance = new SpeechRecognition();
      recognitionInstance.continuous = true;
      recognitionInstance.interimResults = true;
      recognitionInstance.lang = 'ja-JP';
      
      recognitionInstance.onresult = (event: SpeechRecognitionEvent) => {
        let finalTranscript = '';
        let interimTranscript = '';
        
        // Process only new results starting from resultIndex
        // This prevents processing the same results multiple times
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }
        
        // Combine base content with recognition results
        const newContent = baseContentRef.current + finalTranscript + interimTranscript;
        setContent(newContent);
        
        // Update base content when we get final results
        // This ensures the next finalTranscript will be appended, not overwrite
        if (finalTranscript) {
          baseContentRef.current = baseContentRef.current + finalTranscript;
          // éŸ³å£°å…¥åŠ›è‡ªå‹•åœæ­¢: éŸ³å£°ã‚’æ¤œå‡ºã—ãŸã®ã§æ™‚é–“ã‚’æ›´æ–°
          lastSpeechTimeRef.current = Date.now();
        }
      };
      
      recognitionInstance.onerror = (event: SpeechRecognitionErrorEvent) => {
        console.error('Speech recognition error:', event.error);
        if (event.error === 'not-allowed') {
          setError('ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã§ãƒã‚¤ã‚¯ã®ä½¿ç”¨ã‚’è¨±å¯ã—ã¦ãã ã•ã„ã€‚');
          stopRealtimeListening();
        } else if (event.error === 'no-speech') {
          // è‡ªå‹•åœæ­¢è¨­å®šã‚’ãƒã‚§ãƒƒã‚¯
          if (autoStopTimeout !== null) {
            const timeSinceLastSpeech = Date.now() - lastSpeechTimeRef.current;
            if (timeSinceLastSpeech >= autoStopTimeout) {
              console.log(`No speech for ${autoStopTimeout / 1000} seconds, stopping automatically...`);
              stopRealtimeListening();
              return;
            }
          }
          console.log('No speech detected, continuing...');
        } else if (event.error === 'network') {
          console.log('Network error, continuing...');
        } else if (event.error === 'aborted') {
          // abortedã¯é€šå¸¸HMRã‚„ä¸€æ™‚çš„ãªä¸­æ–­ã§ç™ºç”Ÿã™ã‚‹ã®ã§ã€ç¶™ç¶šã‚’è©¦ã¿ã‚‹
          console.log('Speech recognition aborted (possibly due to hot reload), will retry...');
        } else {
          console.log(`Non-critical error: ${event.error}, continuing...`);
        }
      };
      
      recognitionInstance.onstart = () => {
        setIsRealtimeListening(true);
        isRealtimeListeningRef.current = true;
      };
      
      recognitionInstance.onend = () => {
        console.log('Speech recognition ended, isRealtimeListeningRef:', isRealtimeListeningRef.current);
        // Only restart if we're still supposed to be listening
        if (isRealtimeListeningRef.current) {
          setTimeout(() => {
            if (isRealtimeListeningRef.current && recognitionRef.current) {
              try {
                console.log('Attempting to restart speech recognition...');
                recognitionRef.current.start();
              } catch (error) {
                console.error('Failed to restart recognition:', error);
                // If restart fails, stop the real-time listening
                setIsRealtimeListening(false);
                isRealtimeListeningRef.current = false;
                recognitionRef.current = null;
              }
            }
          }, 100);
        } else {
          console.log('Not restarting speech recognition (flag is false)');
        }
      };
      
      // å‚ç…§ã‚’å…ˆã«è¨­å®šã—ã¦ã‹ã‚‰ start ã‚’å‘¼ã¶
      setRecognition(recognitionInstance);
      recognitionRef.current = recognitionInstance;
      
      // ãƒ•ãƒ©ã‚°ã‚’å…ˆã«è¨­å®šï¼ˆstart()ãŒå‘¼ã°ã‚Œã‚‹å‰ã«ï¼‰
      isRealtimeListeningRef.current = true;
      
      // start() ã‚’å‘¼ã¶
      try {
        recognitionInstance.start();
        console.log('Speech recognition started successfully');
      } catch (startError) {
        console.error('Failed to start recognition:', startError);
        // å¤±æ•—ã—ãŸå ´åˆã¯å‚ç…§ã¨ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢
        setRecognition(null);
        recognitionRef.current = null;
        isRealtimeListeningRef.current = false;
        throw startError;
      }
      
    } catch (err) {
      console.error('Failed to start real-time listening:', err);
      setError('éŸ³å£°å…¥åŠ›ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚');
      setIsRealtimeListening(false);
      isRealtimeListeningRef.current = false;
    }
  };

  const stopRealtimeListening = async () => {
    console.log('Stopping real-time listening...');
    
    // First set the flag to false to prevent restart
    isRealtimeListeningRef.current = false;
    setIsRealtimeListening(false);
    
    // éŸ³å£°å…¥åŠ›åœæ­¢æ™‚ã«æœ€çµ‚ä¿å­˜ï¼ˆpendingãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°ï¼‰
    if (content.trim() && currentHearingLogIdRef.current) {
      await autoSaveHearingLog(content, false);
    }
    
    // Stop Azure Speech Service if active
    if (speechProvider === 'azure' && azureSpeech.isListening) {
      azureSpeech.stopListening();
      console.log('Azure Speech Service stopped');
      return;
    }
    
    // Stop Web Speech API if active
    const currentRecognition = recognitionRef.current || recognition;
    if (currentRecognition) {
      try {
        currentRecognition.stop();
        console.log('Web Speech API stopped');
      } catch (error) {
        console.error('Error stopping recognition:', error);
      }
    }
    
    // Clear all references
    setRecognition(null);
    recognitionRef.current = null;
  };

  const handleSave = async () => {
    if (!content.trim()) {
      setError('ãƒ’ã‚¢ãƒªãƒ³ã‚°å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„');
      return;
    }
    setIsSaving(true);
    try {
      if (editingLog) {
        const updated = await hearingApi.updateHearingLog(editingLog.id, { content: content.trim() });
        onHearingLogUpdated?.(updated);
        onCancelEdit?.();
      } else {
        // æ–°è¦ä½œæˆã®å ´åˆ
        if (currentHearingLogIdRef.current) {
          // æ—¢ã«è‡ªå‹•ä¿å­˜ã§ä½œæˆæ¸ˆã¿ã®å ´åˆã¯æ›´æ–°
          const updated = await hearingApi.updateHearingLog(currentHearingLogIdRef.current, { content: content.trim() });
          onHearingLogAdded?.(updated);
        } else {
          // è‡ªå‹•ä¿å­˜ãªã—ã§æ‰‹å‹•ä¿å­˜ã®å ´åˆã¯æ–°è¦ä½œæˆ
          const created = await hearingApi.addHearingLog(projectId, { content: content.trim() });
          onHearingLogAdded?.(created);
        }
        setContent('');
        currentHearingLogIdRef.current = null; // ãƒªã‚»ãƒƒãƒˆ
      }
    } catch (e) {
      console.error(e);
      setError('ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ');
    } finally {
      setIsSaving(false);
    }
  };

  const handleCancel = () => {
    if (editingLog) onCancelEdit?.();
    else setContent('');
  };

  return (
    <div className="space-y-4">
      {/* ============================================ */}
      {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: ãƒˆãƒªã‚¬ãƒ¼è¨­å®šUI */}
      {/* ============================================ */}
      <div className="p-4 bg-gradient-to-r from-green-50 to-blue-50 rounded-lg border border-green-200">
        <h3 className="text-sm font-medium text-gray-700 mb-3 flex items-center">
          <span className="mr-2">âš¡</span>
          ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ãƒ­ãƒ¼ç”Ÿæˆè¨­å®š
        </h3>
        
        <div className="space-y-3">
          <div className="flex items-center space-x-4">
            <label className="text-sm text-gray-600 font-medium">ç”Ÿæˆã‚¿ã‚¤ãƒŸãƒ³ã‚°:</label>
            <select 
              value={triggerSettings.mode}
              onChange={(e) => setTriggerSettings({
                ...triggerSettings,
                mode: e.target.value as TriggerMode
              })}
              className="px-3 py-1.5 border border-gray-300 rounded-md focus:ring-green-500 focus:border-green-500 text-sm"
            >
              <option value="auto">è‡ªå‹•ç”Ÿæˆ</option>
              <option value="manual">æ‰‹å‹•ç”Ÿæˆï¼ˆãƒœã‚¿ãƒ³æŠ¼ä¸‹ï¼‰</option>
            </select>
          </div>

          {triggerSettings.mode === 'auto' && (
            <div className="flex items-center space-x-4">
              <label className="text-sm text-gray-600 font-medium">ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ:</label>
              <select
                value={triggerSettings.timeout || ''}
                onChange={(e) => setTriggerSettings({
                  ...triggerSettings,
                  timeout: parseInt(e.target.value) as AutoTriggerTimeout
                })}
                className="px-3 py-1.5 border border-gray-300 rounded-md focus:ring-green-500 focus:border-green-500 text-sm"
              >
                <option value="2000">2ç§’</option>
                <option value="3000">3ç§’ï¼ˆæ¨å¥¨ï¼‰</option>
                <option value="4000">4ç§’</option>
                <option value="5000">5ç§’</option>
                <option value="6000">6ç§’</option>
                <option value="7000">7ç§’</option>
                <option value="8000">8ç§’</option>
                <option value="9000">9ç§’</option>
                <option value="10000">10ç§’</option>
              </select>
            </div>
          )}

          <div className="flex items-center space-x-4">
            <label className="text-sm text-gray-600 font-medium">éŸ³å£°å…¥åŠ›è‡ªå‹•åœæ­¢:</label>
            <select
              value={autoStopTimeout === null ? 'none' : String(autoStopTimeout)}
              onChange={(e) => setAutoStopTimeout(
                e.target.value === 'none' ? null : parseInt(e.target.value) as AutoStopTimeout
              )}
              className="px-3 py-1.5 border border-gray-300 rounded-md focus:ring-green-500 focus:border-green-500 text-sm"
            >
              <option value="none">åœæ­¢ç„¡ã—</option>
              <option value="60000">1åˆ†ã§åœæ­¢</option>
            </select>
          </div>

          {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ãƒœã‚¿ãƒ³ */}
          {triggerSettings.mode === 'manual' && (
            <button
              onClick={handleManualFlowGeneration}
              disabled={!pendingText.trim() || isGeneratingFlow}
              className="inline-flex items-center px-4 py-2 bg-green-600 text-white rounded-md hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
            >
              {isGeneratingFlow ? (
                <>
                  <svg className="animate-spin -ml-1 mr-2 h-4 w-4 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                    <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                    <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                  </svg>
                  ãƒ•ãƒ­ãƒ¼ç”Ÿæˆä¸­...
                </>
              ) : (
                <>
                  <span className="mr-2">ğŸ“Š</span>
                  ãƒ•ãƒ­ãƒ¼ã‚’ç”Ÿæˆ
                </>
              )}
            </button>
          )}

          {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: ç”Ÿæˆä¸­ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ */}
          {isGeneratingFlow && (
            <div className="flex items-center space-x-2 text-green-700 bg-green-100 px-3 py-2 rounded-md">
              <div className="w-2 h-2 bg-green-500 rounded-full animate-pulse"></div>
              <span className="text-sm font-medium">ãƒ•ãƒ­ãƒ¼ã‚’ç”Ÿæˆä¸­...</span>
            </div>
          )}
        </div>
      </div>
      {/* ============================================ */}

      <div>
        <label htmlFor="hearing-input" className="block text-sm font-medium text-gray-700 mb-2">
          {isEditing ? 'ãƒ’ã‚¢ãƒªãƒ³ã‚°å†…å®¹ã‚’ç·¨é›†' : 'æ–°ã—ã„ãƒ’ã‚¢ãƒªãƒ³ã‚°å†…å®¹'}
        </label>
        
        {/* Voice Input Controls */}
        <div className="mb-3 p-3 bg-gray-50 rounded-lg border border-gray-200">
          <div className="flex items-center justify-between mb-2">
            <h3 className="text-sm font-medium text-gray-700">éŸ³å£°å…¥åŠ›</h3>
            <div className="flex items-center space-x-2">
              {/* Speech Provider Display */}
              <span className="text-xs text-gray-500">
                {speechProvider === 'azure' ? (
                  azureSpeech.isAvailable ? (
                    <span className="inline-flex items-center px-2 py-1 bg-blue-100 text-blue-700 rounded">
                      <svg className="w-3 h-3 mr-1" fill="currentColor" viewBox="0 0 20 20">
                        <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586 7.707 9.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clipRule="evenodd" />
                      </svg>
                      Azure Speech
                    </span>
                  ) : (
                    <span className="inline-flex items-center px-2 py-1 bg-yellow-100 text-yellow-700 rounded">
                      <svg className="w-3 h-3 mr-1" fill="currentColor" viewBox="0 0 20 20">
                        <path fillRule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clipRule="evenodd" />
                      </svg>
                      Azure æœªè¨­å®š
                    </span>
                  )
                ) : (
                  <span className="inline-flex items-center px-2 py-1 bg-gray-100 text-gray-700 rounded">
                    Web Speech API
                  </span>
                )}
              </span>
              {/* Provider Switch Button */}
              {!isRealtimeListening && (
                <button
                  onClick={() => setSpeechProvider(speechProvider === 'azure' ? 'web' : 'azure')}
                  className="text-xs px-2 py-1 text-blue-600 hover:text-blue-800 hover:bg-blue-50 rounded transition-colors"
                  title={`${speechProvider === 'azure' ? 'Web Speech API' : 'Azure Speech'}ã«åˆ‡ã‚Šæ›¿ãˆ`}
                >
                  åˆ‡æ›¿
                </button>
              )}
            </div>
          </div>
          
          <div className="flex items-center space-x-3">
            {!(isRealtimeListening || azureSpeech.isListening) ? (
                <button
                  onClick={startRealtimeListening}
                  disabled={isSaving || isTranscribing}
                  className="inline-flex items-center px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
                  title="éŸ³å£°å…¥åŠ›ã‚’é–‹å§‹"
                >
                  <svg className="w-4 h-4 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                  </svg>
                  éŸ³å£°å…¥åŠ›
                </button>
            ) : (
              <div className="flex items-center space-x-2">
                <button
                  onClick={stopRealtimeListening}
                  className="inline-flex items-center px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 transition-colors"
                >
                  <svg className="w-4 h-4 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 10a1 1 0 011-1h4a1 1 0 011 1v4a1 1 0 01-1 1h-4a1 1 0 01-1-1v-4z" />
                  </svg>
                  éŸ³å£°å…¥åŠ›åœæ­¢
                </button>
              </div>
            )}
            
            {(isRealtimeListening || azureSpeech.isListening) && (
              <div className="flex items-center space-x-3">
                <div className="flex items-center">
                  <div className="w-3 h-3 bg-blue-500 rounded-full animate-pulse"></div>
                  <span className="ml-2 text-sm text-blue-600">
                    {speechProvider === 'azure' ? 'Azure Speech èªè­˜ä¸­...' : 'Web Speech èªè­˜ä¸­...'}
                  </span>
                </div>
              </div>
            )}
            
            {azureSpeech.error && (
              <div className="text-sm text-red-600">
                {azureSpeech.error}
              </div>
            )}
            
            {isTranscribing && (
              <div className="flex items-center space-x-2">
                <svg className="animate-spin w-4 h-4 text-blue-600" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                  <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                  <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                </svg>
                <span className="text-sm text-blue-600">æ–‡å­—èµ·ã“ã—ä¸­...</span>
              </div>
            )}
          </div>
          
          {/* Instructions */}
          {!isTranscribing && !isRealtimeListening && (
            <div className="mt-2 text-xs text-gray-500">
              ğŸ’¡ ãƒ’ãƒ³ãƒˆ: éŸ³å£°å…¥åŠ›ã§è©±ã—ãŸå†…å®¹ãŒå³åº§ã«ãƒ†ã‚­ã‚¹ãƒˆã«åæ˜ ã•ã‚Œã¾ã™ã€‚é™ã‹ãªç’°å¢ƒã§è¡Œã†ã¨ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚
            </div>
          )}
          
          {isRealtimeListening && (
            <div className="mt-2 text-xs text-blue-600">
              ğŸ¤ éŸ³å£°å…¥åŠ›ä¸­: è©±ã—ãŸå†…å®¹ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ†ã‚­ã‚¹ãƒˆã«åæ˜ ã•ã‚Œã¾ã™ã€‚
            </div>
          )}
        </div>
        
        <textarea
          id="hearing-input"
          rows={10}
          value={content}
          onChange={(e) => setContent(e.target.value)}
          className="w-full px-3 py-2 border border-gray-300 rounded-md focus:ring-blue-500 focus:border-blue-500"
          placeholder="ãƒ’ã‚¢ãƒªãƒ³ã‚°å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆéŸ³å£°å…¥åŠ›ãƒœã‚¿ãƒ³ã§éŸ³å£°ã‹ã‚‰ã®æ–‡å­—èµ·ã“ã—ã‚‚å¯èƒ½ã§ã™ï¼‰"
          disabled={isTranscribing || isRealtimeListening}
        />
        
        {/* ============================================ */}
        {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆç”¨: æœªå‡¦ç†ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º */}
        {/* ============================================ */}
        {pendingText && (
          <div className="mt-3 p-3 bg-yellow-50 border-l-4 border-yellow-400 rounded-md">
            <div className="flex items-start">
              <span className="text-yellow-600 mr-2">ğŸ“</span>
              <div className="flex-1">
                <p className="text-xs font-medium text-yellow-800 mb-1">
                  ãƒ•ãƒ­ãƒ¼ç”Ÿæˆå¾…ã¡ã®ãƒ†ã‚­ã‚¹ãƒˆ:
                </p>
                <p className="text-sm text-gray-700 font-medium leading-relaxed">
                  {pendingText}
                </p>
              </div>
            </div>
          </div>
        )}
        {/* ============================================ */}

      </div>

      {error && (
        <div className="p-3 bg-red-50 border border-red-200 rounded-md">
          <p className="text-red-700 text-sm">{error}</p>
        </div>
      )}

      <div className="flex justify-end space-x-2">
        {isEditing && (
          <button 
            onClick={handleCancel} 
            disabled={isSaving || isTranscribing || isRealtimeListening} 
            className="px-4 py-2 bg-gray-500 text-white rounded-md hover:bg-gray-600 focus:outline-none focus:ring-2 focus:ring-gray-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
          >
            ã‚­ãƒ£ãƒ³ã‚»ãƒ«
          </button>
        )}
        <button 
          onClick={handleSave} 
          disabled={isSaving || !content.trim() || isTranscribing || isRealtimeListening} 
          className="px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
        >
          {isSaving ? 'ä¿å­˜ä¸­...' : isEditing ? 'æ›´æ–°' : 'ä¿å­˜'}
        </button>
      </div>
    </div>
  );
}

export default HearingInput;